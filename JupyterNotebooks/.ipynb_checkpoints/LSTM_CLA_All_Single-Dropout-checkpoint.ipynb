{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.    -0.    -0.   ...  23.8   10.74   0.76]\n",
      " [ -0.    -0.    -0.   ... -28.4  -37.39 -47.95]\n",
      " [ -0.    -0.    -0.   ...   4.31   5.51   3.66]\n",
      " ...\n",
      " [ -0.    -0.    -0.   ...   6.99   5.57   3.68]\n",
      " [ -0.    -0.    -0.   ...  -8.31  -9.34  -7.32]\n",
      " [ -0.    -0.    -0.   ...  -6.    -5.99  -4.9 ]]\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "## Construct LSTM using Tensorflow + Keras\n",
    "# Import Libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################################################################\n",
    "######################################################################\n",
    "## CSP File from: https://github.com/spolsley/common-spatial-patterns\n",
    "\n",
    "# CSP takes any number of arguments, but each argument must be a collection of trials associated with a task\n",
    "# That is, for N tasks, N arrays are passed to CSP each with dimensionality (# of trials of task N) x (feature vector)\n",
    "# Trials may be of any dimension, provided that each trial for each task has the same dimensionality,\n",
    "# otherwise there can be no spatial filtering since the trials cannot be compared\n",
    "def CSP(*tasks):\n",
    "\tif len(tasks) < 2:\n",
    "\t\tprint(\"Must have at least 2 tasks for filtering.\")\n",
    "\t\treturn (None,) * len(tasks)\n",
    "\telse:\n",
    "\t\tfilters = ()\n",
    "\t\t# CSP algorithm\n",
    "\t\t# For each task x, find the mean variances Rx and not_Rx, which will be used to compute spatial filter SFx\n",
    "\t\titerator = range(0,len(tasks))\n",
    "\t\tfor x in iterator:\n",
    "\t\t\t# Find Rx\n",
    "\t\t\tRx = covarianceMatrix(tasks[x][0])\n",
    "\t\t\tfor t in range(1,len(tasks[x])):\n",
    "\t\t\t\tRx += covarianceMatrix(tasks[x][t])\n",
    "\t\t\tRx = Rx / len(tasks[x])\n",
    "\n",
    "\t\t\t# Find not_Rx\n",
    "\t\t\tcount = 0\n",
    "\t\t\tnot_Rx = Rx * 0\n",
    "\t\t\tfor not_x in [element for element in iterator if element != x]:\n",
    "\t\t\t\tfor t in range(0,len(tasks[not_x])):\n",
    "\t\t\t\t\tnot_Rx += covarianceMatrix(tasks[not_x][t])\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\tnot_Rx = not_Rx / count\n",
    "\n",
    "\t\t\t# Find the spatial filter SFx\n",
    "\t\t\tSFx = spatialFilter(Rx,not_Rx)\n",
    "\t\t\tfilters += (SFx,)\n",
    "\n",
    "\t\t\t# Special case: only two tasks, no need to compute any more mean variances\n",
    "\t\t\tif len(tasks) == 2:\n",
    "\t\t\t\tfilters += (spatialFilter(not_Rx,Rx),)\n",
    "\t\t\t\tbreak\n",
    "\t\treturn filters\n",
    "\n",
    "# covarianceMatrix takes a matrix A and returns the covariance matrix, scaled by the variance\n",
    "def covarianceMatrix(A):\n",
    "\tCa = np.dot(A,np.transpose(A))/np.trace(np.dot(A,np.transpose(A)))\n",
    "\treturn Ca\n",
    "\n",
    "# spatialFilter returns the spatial filter SFa for mean covariance matrices Ra and Rb\n",
    "def spatialFilter(Ra,Rb):\n",
    "\tR = Ra + Rb\n",
    "\tE,U = la.eig(R)\n",
    "\n",
    "\t# CSP requires the eigenvalues E and eigenvector U be sorted in descending order\n",
    "\tord = np.argsort(E)\n",
    "\tord = ord[::-1] # argsort gives ascending order, flip to get descending\n",
    "\tE = E[ord]\n",
    "\tU = U[:,ord]\n",
    "\n",
    "\t# Find the whitening transformation matrix\n",
    "\tP = np.dot(np.sqrt(la.inv(np.diag(E))),np.transpose(U))\n",
    "\n",
    "\t# The mean covariance matrices may now be transformed\n",
    "\tSa = np.dot(P,np.dot(Ra,np.transpose(P)))\n",
    "\tSb = np.dot(P,np.dot(Rb,np.transpose(P)))\n",
    "\n",
    "\t# Find and sort the generalized eigenvalues and eigenvector\n",
    "\tE1,U1 = la.eig(Sa,Sb)\n",
    "\tord1 = np.argsort(E1)\n",
    "\tord1 = ord1[::-1]\n",
    "\tE1 = E1[ord1]\n",
    "\tU1 = U1[:,ord1]\n",
    "\n",
    "\t# The projection matrix (the spatial filter) may now be obtained\n",
    "\tSFa = np.dot(np.transpose(U1),P)\n",
    "\treturn SFa.astype(np.float32)\n",
    "######################################################################\n",
    "def GetMinSteps(indeces, data):\n",
    "\tminVal = 9999;\n",
    "\tfor index in indeces:\n",
    "\t\tlength = data[index].shape[1];\n",
    "\t\tif(length < minVal):\n",
    "\t\t\tminVal = length;\n",
    "\treturn minVal\n",
    "######################################################################\n",
    "def GetData(indeces, dataIn, truncateValue):\n",
    "\tdataOut = []\n",
    "\ttruncate = True;\n",
    "\tif truncateValue == 0:\n",
    "\t\ttruncate = False;\n",
    "\tfor idx in indeces:\n",
    "\t\tcurrentData = dataIn[idx]\n",
    "\t\tif truncate:\n",
    "\t\t\tdataOut.append(currentData[:,0:truncateValue])\n",
    "\t\telse:\n",
    "\t\t\tdataOut.append(currentData)\n",
    "\treturn np.asarray(dataOut)\n",
    "######################################################################\n",
    "######################################################################\n",
    "######################################################################\n",
    "enableDropout = True;\n",
    "dropoutPercentage = 0.3;\n",
    "\n",
    "fileNames = ['CLASubjectA1601083StLRHand.mat',\\\n",
    "             'CLASubjectB1510193StLRHand.mat',\\\n",
    "             'CLASubjectB1510203StLRHand.mat',\\\n",
    "             'CLASubjectB1512153StLRHand.mat',\\\n",
    "             'CLASubjectC1511263StLRHand.mat',\\\n",
    "             'CLASubjectC1512163StLRHand.mat',\\\n",
    "             'CLASubjectC1512233StLRHand.mat',\\\n",
    "             'CLASubjectD1511253StLRHand.mat',\\\n",
    "             'CLASubjectE1512253StLRHand.mat',\\\n",
    "             'CLASubjectE1601193StLRHand.mat',\\\n",
    "             'CLASubjectE1601223StLRHand.mat',\\\n",
    "             'CLASubjectF1509163StLRHand.mat',\\\n",
    "             'CLASubjectF1509173StLRHand.mat',\\\n",
    "             'CLASubjectF1509283StLRHand.mat'];\n",
    "\n",
    "numDatasets = len(fileNames);\n",
    "\n",
    "testAccuracyNoCSP = np.zeros((1,numDatasets))\n",
    "trainAccuracyNoCSP = np.zeros((1,numDatasets))\n",
    "testAccuracyWithCSP = np.zeros((1,numDatasets))\n",
    "trainAccuracyWithCSP = np.zeros((1,numDatasets))\n",
    "\n",
    "for dataset in range(0, numDatasets):\n",
    "\n",
    "\tfileName = fileNames[dataset];\n",
    "\n",
    "\tfile = sio.loadmat('../../../matDown/CLA_Data/{}'.format(fileName)) #replace with .mat file name\n",
    "\theader=file['__header__']\n",
    "\tversion=file['__version__']\n",
    "\tglob=file['__globals__']\n",
    "\t#ans=file['ans']\n",
    "\n",
    "\n",
    "\t#x=file['x']\n",
    "\to=file['o'][0][0]\n",
    "\tdata=o['data']\n",
    "\tdata = np.transpose(data)\n",
    "\tdata = data[0:21,:];\n",
    "\tprint(data)\n",
    "\tnS=o['nS'][0][0]\n",
    "\t#values of structure seem to be 2D numpy arrays, if originally a scalar in Matlab.\n",
    "\t#use [0][0] to get scalar.\n",
    "\t#print(\"Number of samples: {numSamples}\".format(numSamples=nS))\n",
    "\ttest=o['id'][0] #id value became a 1D array of size 1 for some reason. use [0] to get value\n",
    "\t#print(\"Dataset ID: {id}\".format(id=test))\n",
    "\tchnames=o['chnames'][:,0] #[:,0] converts from 2D array back to 1D array\n",
    "\t#print(\"Channel names: {channelNames}\".format(channelNames=chnames))\n",
    "\tmarkers = o['marker']\n",
    "\t## The markers are all still individual arrays of size 1x1, so we convert them to an array with single values\n",
    "\tmarkersArray = []\n",
    "\tfor marker in markers:\n",
    "\t\tmarkersArray.append(marker[0])\n",
    "\tmarkersArray = np.asarray(markersArray)\n",
    "\n",
    "\t#################################\n",
    "\t#################################\n",
    "\t#5F interaction paradigm\n",
    "\t#1-thumb MI, 2-index finger MI, 3-middle finger MI, 4-ring finger MI, 5-pinkie finger MI\n",
    "\n",
    "\t#all paradigms\n",
    "\t#99-initial relaxation period\n",
    "\t#91-inter-session rest break period\n",
    "\t#92-experiment end\n",
    "\t#################################\n",
    "\n",
    "\t## Find the starting indeces where the marker changes\n",
    "\tchangeIdxs = np.where(np.transpose(markersArray)[:-1] != np.transpose(markersArray)[1:])[0]\n",
    "\t#print(\"Number of index changes: {idxChanges}\".format(idxChanges=changeIdxs.shape[0]))\n",
    "\t## Split the data so that it has its matching marker\n",
    "\tdataSplit = np.array_split(data, changeIdxs[:-1], axis=1)\n",
    "\tsplitCount = 0\n",
    "\tfor splitData in dataSplit:\n",
    "\t\tsplitCount += 1\n",
    "\t#print(\"Number of arrays in data split: {num}\".format(num=splitCount))\n",
    "\t## Retrieve the marker values for each of the change indeces (changeIdxs)\n",
    "\tmarkerTargets = markersArray[changeIdxs];\n",
    "\t#print(\"Number of marker targets: {numTargets}\".format(numTargets=markerTargets.shape[0]))\n",
    "\n",
    "\t## To Apply CSP, we first only get the indeces for MI tasks 1 and 2 (left and right hand, respectively.)\n",
    "\tlhIdx = np.where(markerTargets == 1)[0]\n",
    "\trhIdx = np.where(markerTargets == 2)[0]\n",
    "\t#llIdx = np.where(markerTargets == 4)[0]\n",
    "\t#tIdx = np.where(markerTargets == 5)[0]\n",
    "\t#rlIdx = np.where(markerTargets == 6)[0]\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t#tCount = tIdx.shape\n",
    "\t#print(\"Thumb Marker Count: {}\\tSize of First: ({},{})\".format(tCount, dataSplit[tIdx[0]].shape[0],dataSplit[tIdx[0]].shape[1]))\n",
    "\t#print(\"Thumb Marker Count: {}\\tSize of Second: ({},{})\".format(tCount, dataSplit[tIdx[1]].shape[0],dataSplit[tIdx[1]].shape[1]))\n",
    "\t#print(\"Thumb Marker Count: {}\\tSize of Third: ({},{})\".format(tCount, dataSplit[tIdx[2]].shape[0],dataSplit[tIdx[2]].shape[1]))\n",
    "\n",
    "\n",
    "\tlhIdxMin = GetMinSteps(lhIdx, dataSplit)\n",
    "\trhIdxMin = GetMinSteps(rhIdx, dataSplit)\n",
    "\t#llIdxMin = GetMinSteps(llIdx, dataSplit)\n",
    "\t#tIdxMin = GetMinSteps(tIdx, dataSplit)\n",
    "\t#rlIdxMin = GetMinSteps(rlIdx, dataSplit)\n",
    "\t#minValues = [lhIdxMin, rhIdxMin, llIdxMin, tIdxMin, rlIdxMin]\n",
    "\tminValues = [lhIdxMin, rhIdxMin]\n",
    "\t#minValues\n",
    "\n",
    "\t#Truncate the data to the min size\n",
    "\tminValue = np.min(minValues)\n",
    "\tprint(minValue)\n",
    "\n",
    "\tlhData = GetData(lhIdx, dataSplit, minValue)\n",
    "\trhData = GetData(rhIdx, dataSplit, minValue)\n",
    "\t#llData = GetData(llIdx, dataSplit, minValue)\n",
    "\t#tData = GetData(tIdx, dataSplit, minValue)\n",
    "\t#rlData = GetData(rlIdx, dataSplit, minValue)\n",
    "\t#print(\"Length of tData: {}\".format(len(tData)))\n",
    "\t#print(\"Length of iData: {}\".format(len(iData)))\n",
    "\t#print(\"Length of mData: {}\".format(len(mData)))\n",
    "\t#print(\"Length of rData: {}\".format(len(rData)))\n",
    "\t#print(\"Length of pData: {}\".format(len(pData)))\n",
    "\n",
    "\t#minLen = np.min([len(lhData), len(rhData), len(llData), len(tData), len(rlData)])\n",
    "\tminLen = np.min([len(lhData), len(rhData)])\n",
    "\t##Want to make sure they are balanced, so we keep the minLen values\n",
    "\tlhData = lhData[0:minLen]\n",
    "\trhData = rhData[0:minLen]\n",
    "\t#llData = llData[0:minLen]\n",
    "\t#tData = tData[0:minLen]\n",
    "\t#rlData = rlData[0:minLen]\n",
    "\n",
    "\n",
    "\t# In[68]:\n",
    "\n",
    "\n",
    "\t#Construct the target arrays and merge the data\n",
    "\tlhTargets = np.tile(np.array([1,0]),(minLen,1))\n",
    "\trhTargets = np.tile(np.array([0,1]),(minLen,1))\n",
    "\t#llTargets = np.tile(np.array([0,0,1,0,0]),(minLen,1))\n",
    "\t#tTargets = np.tile(np.array([0,0,0,1,0]),(minLen,1))\n",
    "\t#rlTargets = np.tile(np.array([0,0,0,0,1]),(minLen,1))\n",
    "    \n",
    "\t#Print the sizes of each data\n",
    "\t#print(\"lhData shape: {}\".format(lhData.shape))\n",
    "\n",
    "\t#markerTargets = np.vstack((lhTargets, rhTargets, llTargets, tTargets, rlTargets))\n",
    "\tmarkerTargets = np.vstack((lhTargets, rhTargets))\n",
    "\t#Data = np.vstack((lhData, rhData, llData, tData, rlData))\n",
    "\tData = np.vstack((lhData, rhData))\n",
    "    \n",
    "\t#Sanity Check\n",
    "\t#print(\"Data Shape: {arg1}\\tmarkerTargets Shape: {arg2}\".format(arg1=Data.shape, arg2=markerTargets.shape))\n",
    "\n",
    "\t## Shuffle the data\n",
    "\tData, markerTargets = shuffle(Data, markerTargets, random_state=0)\n",
    "\n",
    "\t## Split into train and test sets\n",
    "\tDataTrain, DataTest, markerTargetsTrain, markerTargetsTest = train_test_split(Data, markerTargets, test_size=0.3, random_state=1)\n",
    "\tmarkerTargetsTrain.shape\n",
    "\n",
    "\t## Reshape the data for time-series processing\n",
    "\t## Syntax np.reshape((numExamples, numTimeSteps, numInputs/numFeatures))\n",
    "\tDataTrainRe = DataTrain.reshape((DataTrain.shape[0], DataTrain.shape[1], DataTrain.shape[2]))\n",
    "\tDataTestRe = DataTest.reshape((DataTest.shape[0], DataTest.shape[1], DataTest.shape[2]))\n",
    "\n",
    "\t## Construct the model\n",
    "\tLSTM_EEG = Sequential()\n",
    "\tLSTM_EEG.add(LSTM((100),batch_input_shape=(None,DataTrainRe.shape[1], DataTrainRe.shape[2]), return_sequences=True))\n",
    "\tif(enableDropout):\n",
    "\t\tLSTM_EEG.add(Dropout(dropoutPercentage))\n",
    "\tLSTM_EEG.add(LSTM((50), return_sequences=False))\n",
    "\tif(enableDropout):\n",
    "\t\tLSTM_EEG.add(Dropout(dropoutPercentage))\n",
    "\tLSTM_EEG.add(Dense((2),activation='sigmoid'))\n",
    "\n",
    "\tLSTM_EEG.summary()\n",
    "\tsgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\tLSTM_EEG.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\t# In[79]:\n",
    "\n",
    "\n",
    "\thistory = LSTM_EEG.fit(DataTrain, markerTargetsTrain, epochs=30,verbose=2, batch_size=16)\n",
    "\n",
    "\n",
    "\t# In[80]:\n",
    "\n",
    "\n",
    "\tpredictionsTest = LSTM_EEG.predict(DataTest)\n",
    "\n",
    "\n",
    "\t# In[81]:\n",
    "\n",
    "\n",
    "\tpredictionsTest[predictionsTest>0.5] = 1\n",
    "\n",
    "\n",
    "\t# In[82]:\n",
    "\n",
    "\n",
    "\tpredictionsTest[predictionsTest <= 0.5] = 0\n",
    "\n",
    "\n",
    "\t# In[83]:\n",
    "\n",
    "\n",
    "\tcomparisonArrayTest = predictionsTest == markerTargetsTest\n",
    "\n",
    "\n",
    "\t# In[85]:\n",
    "\n",
    "\n",
    "\tcorrectCountTest = 0\n",
    "\tfor boolValues in comparisonArrayTest:\n",
    "\t\tif(boolValues[0] & boolValues[1]):\n",
    "\t\t\tcorrectCountTest += 1\n",
    "\tfalseCountTest = DataTest.shape[0] - correctCountTest\n",
    "\n",
    "\tpredictionsTrain = LSTM_EEG.predict(DataTrain)\n",
    "\tpredictionsTrain[predictionsTrain>0.5] = 1;\n",
    "\tpredictionsTrain[predictionsTrain<=0.5] = 0;\n",
    "\tcomparisonArrayTrain = predictionsTrain == markerTargetsTrain;\n",
    "\n",
    "\tcorrectCountTrain = 0\n",
    "\tfor boolValues in comparisonArrayTrain:\n",
    "\t\tif(boolValues[0] & boolValues[1]):\n",
    "\t\t\tcorrectCountTrain += 1\n",
    "\tfalseCountTrain = DataTrain.shape[0] - correctCountTrain\n",
    "\n",
    "\n",
    "\t# In[87]:\n",
    "\ttrainAcc_noCSP = (correctCountTrain*100/DataTrain.shape[0]);\n",
    "\ttestAcc_noCSP = (correctCountTest*100/DataTest.shape[0]);\n",
    "\n",
    "\t#Save these values into the array\n",
    "\ttrainAccuracyNoCSP[0,dataset] = trainAcc_noCSP;\n",
    "\ttestAccuracyNoCSP[0, dataset] = testAcc_noCSP;\n",
    "\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"Training Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTrain, falseCountTrain, trainAcc_noCSP))\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"Testing Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTest, falseCountTest, testAcc_noCSP))\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"#################################\")\n",
    "\n",
    "\n",
    "\t# In[88]:\n",
    "\n",
    "\n",
    "\t## Applying CSP to 5F data\n",
    "\t#filters = CSP(lhData, rhData, llData, tData, rlData)\n",
    "\tfilters = CSP(lhData, rhData)\n",
    "\n",
    "\t# In[90]:\n",
    "\n",
    "\n",
    "\tfiltersArray = np.asarray(filters)\n",
    "\n",
    "\n",
    "\t# In[91]:\n",
    "\n",
    "\n",
    "\tfiltersArray.shape\n",
    "\n",
    "\n",
    "\t# In[92]:\n",
    "\n",
    "\n",
    "\tlhData_CSP = np.matmul(np.transpose(filtersArray[0]), lhData)\n",
    "\trhData_CSP = np.matmul(np.transpose(filtersArray[1]), rhData)\n",
    "\t#llData_CSP = np.matmul(np.transpose(filtersArray[2]), llData)\n",
    "\t#tData_CSP = np.matmul(np.transpose(filtersArray[3]), tData)\n",
    "\t#rlData_CSP = np.matmul(np.transpose(filtersArray[4]), rlData)\n",
    "\n",
    "\n",
    "\t# In[93]:\n",
    "\n",
    "\n",
    "\t#Data_CSP = np.vstack((lhData_CSP, rhData_CSP, llData_CSP, tData_CSP, rlData_CSP))\n",
    "\tData_CSP = np.vstack((lhData_CSP, rhData_CSP))\n",
    "\n",
    "\n",
    "\t# In[94]:\n",
    "\n",
    "\n",
    "\t#Construct the target arrays and merge the data\n",
    "\tlhTargets = np.tile(np.array([1,0]),(minLen,1))\n",
    "\trhTargets = np.tile(np.array([0,1]),(minLen,1))\n",
    "\t#llTargets = np.tile(np.array([0,0,1,0,0]),(minLen,1))\n",
    "\t#tTargets = np.tile(np.array([0,0,0,1,0]),(minLen,1))\n",
    "\t#rlTargets = np.tile(np.array([0,0,0,0,1]),(minLen,1))\n",
    "\n",
    "\t#markerTargets = np.vstack((lhTargets, rhTargets, llTargets, tTargets, rlTargets))\n",
    "\tmarkerTargets = np.vstack((lhTargets, rhTargets))\n",
    "\n",
    "\t# In[95]:\n",
    "\n",
    "\n",
    "\t## Shuffle the data\n",
    "\tData_CSP, markerTargets_CSP = shuffle(Data_CSP, markerTargets, random_state=0)\n",
    "\t## Split into train and test sets\n",
    "\tDataTrain_CSP, DataTest_CSP, markerTargetsTrain_CSP, markerTargetsTest_CSP = train_test_split(Data_CSP, markerTargets_CSP, test_size=0.3, random_state=1)\n",
    "\n",
    "\t##Use the same model to train and test\n",
    "\thistory_CSP = LSTM_EEG.fit(DataTrain_CSP, markerTargetsTrain_CSP, epochs=30,verbose=2, batch_size=16)\n",
    "\n",
    "\n",
    "\t# In[96]:\n",
    "\n",
    "\n",
    "\tpredictionsTest = LSTM_EEG.predict(DataTest_CSP)\n",
    "\n",
    "\n",
    "\t# In[97]:\n",
    "\n",
    "\n",
    "\tpredictionsTest[predictionsTest>0.5] = 1\n",
    "\n",
    "\n",
    "\t# In[98]:\n",
    "\n",
    "\n",
    "\tpredictionsTest[predictionsTest <= 0.5] = 0\n",
    "\n",
    "\n",
    "\t# In[99]:\n",
    "\n",
    "\n",
    "\tcomparisonArrayTest = predictionsTest == markerTargetsTest\n",
    "\n",
    "\n",
    "\t# In[100]:\n",
    "\n",
    "\n",
    "\tcorrectCountTest = 0\n",
    "\tfor boolValues in comparisonArrayTest:\n",
    "\t\tif(boolValues[0] & boolValues[1]):\n",
    "\t\t\tcorrectCountTest += 1\n",
    "\tfalseCountTest = DataTest_CSP.shape[0] - correctCountTest\n",
    "\n",
    "\tpredictionsTrain = LSTM_EEG.predict(DataTrain_CSP)\n",
    "\tpredictionsTrain[predictionsTrain>0.5] = 1;\n",
    "\tpredictionsTrain[predictionsTrain<=0.5] = 0;\n",
    "\tcomparisonArrayTrain = predictionsTrain == markerTargetsTrain;\n",
    "\n",
    "\tcorrectCountTrain = 0\n",
    "\tfor boolValues in comparisonArrayTrain:\n",
    "\t\tif(boolValues[0] & boolValues[1]):\n",
    "\t\t\tcorrectCountTrain += 1\n",
    "\tfalseCountTrain = DataTrain_CSP.shape[0] - correctCountTrain\n",
    "\n",
    "\t#Computing the accuracy\n",
    "\ttrainAcc_wCSP = (correctCountTrain*100/DataTrain_CSP.shape[0]);\n",
    "\ttestAcc_wCSP = (correctCountTest*100/DataTest_CSP.shape[0]);\n",
    "\n",
    "\t#Save these values into the array\n",
    "\ttrainAccuracyWithCSP[0,dataset] = trainAcc_wCSP;\n",
    "\ttestAccuracyWithCSP[0, dataset] = testAcc_wCSP;\n",
    "\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"### RESULTS AFTER APPLYING CSP ##\")\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"Training Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTrain, falseCountTrain, trainAcc_wCSP))\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"Testing Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTest, falseCountTest, testAcc_wCSP))\n",
    "\tprint(\"#################################\")\n",
    "\tprint(\"#################################\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAccuracyNoCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAccuracyWithCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAccuracyNoCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAccuracyWithCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedArray = np.transpose(np.asarray(np.vstack((trainAccuracyNoCSP,testAccuracyNoCSP,trainAccuracyWithCSP, testAccuracyWithCSP))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('CLAperSubject.csv', combinedArray, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
