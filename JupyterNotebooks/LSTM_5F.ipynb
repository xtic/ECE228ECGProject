{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two-Class EEG Classification using LSTM-based RNN\n",
    "## Here, we develop a baseline model for using an RNN to perform time-series prediction for one of two motor\n",
    "## imagery (MI) tasks. The data from the file CLA_SubjectJ-170508-3St-LRHand-Inter.mat is processed and we\n",
    "## only keep the data points that pertain to the left and right hand MI tasks. Each of the MI task data consists\n",
    "## of 170 time steps, with each time step consisting of 22 inputs. In other words, we pass the current sample for\n",
    "## of the 22 channels for each time step. At the end of the 170 time step, we send the output of the LSTM layers\n",
    "## to a Dense layer with two outputs. To ease the classification at the outer layer, we have converted the class\n",
    "## into a one-hot encoding of zeros and ones, and use the sigmoid unit at the output to match the output range.\n",
    "\n",
    "## Preliminary results:\n",
    "## Performance on training set is 100% classification rate after around 30 to 40 epochs.\n",
    "## Performance on test set is around 60% - 68%.\n",
    "\n",
    "## Potential problems:\n",
    "## The total amount of data is relatively low. Can it be possible to combine multiple subject's L/R Hand MI data\n",
    "## into one unified data set?\n",
    "## Can also possible increase the size of the data set by doing some data augmentations.\n",
    "## For example, injecting some noise into the existing EEG data to create more test data?\n",
    "\n",
    "## Followup/TODO:\n",
    "## Use pre-implemented common spatial patterns for feature extraction: avialable here: https://github.com/spolsley/common-spatial-patterns\n",
    "## Try using a relatively simple non-linear energy operator as a pre-emphasis step on the data to see if it helps generalize\n",
    "## the model.\n",
    "\n",
    "\n",
    "### NOTE: 05.26.2020\n",
    "## This notebook is incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file is used to split data into series of arrays and their corresponding MI task.\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataDir = \"../../../matDown/5F_Data/\" #replace with folder the 5F files are in\n",
    "dataList = []\n",
    "markersArrayList=[]\n",
    "for file in os.listdir( dataDir ) : #loads all 5F mat files\n",
    "    temp=sio.loadmat(dataDir+file)\n",
    "    tempO=temp['o'][0][0]\n",
    "    tempData=tempO['data']\n",
    "    tempData=np.transpose(tempData)\n",
    "    tempData=tempData[0:21,:] #ignore 22nd channel\n",
    "    dataList.append(tempData)\n",
    "    tempMarkers=tempO['marker']\n",
    "    tempMarkersArray = []\n",
    "    for tempMarker in tempMarkers:\n",
    "        tempMarkersArray.append(tempMarker[0])\n",
    "    tempMarkersArray = np.asarray(tempMarkersArray)\n",
    "    markersArrayList.append(tempMarkersArray)\n",
    "data=np.concatenate(dataList, axis=1)\n",
    "markersArray=np.concatenate(markersArrayList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: #for 1 loading just 1 .mat file\n",
    "    ## This file is used to split data into series of arrays and their corresponding MI task.\n",
    "    import scipy.io as sio\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    file = sio.loadmat('../mat_files/CLA-SubjectJ-170508-3St-LRHand-Inter.mat') #replace with .mat file name\n",
    "    header=file['__header__']\n",
    "    version=file['__version__']\n",
    "    glob=file['__globals__']\n",
    "    ans=file['ans']\n",
    "\n",
    "\n",
    "    x=file['x']\n",
    "    o=file['o'][0][0]\n",
    "    data=o['data']\n",
    "    data = np.transpose(data)\n",
    "    print(data)\n",
    "    nS=o['nS'][0][0]\n",
    "    #values of structure seem to be 2D numpy arrays, if originally a scalar in Matlab.\n",
    "    #use [0][0] to get scalar.\n",
    "    print(\"Number of samples: {numSamples}\".format(numSamples=nS))\n",
    "    test=o['id'][0] #id value became a 1D array of size 1 for some reason. use [0] to get value\n",
    "    print(\"Dataset ID: {id}\".format(id=test))\n",
    "    chnames=o['chnames'][:,0] #[:,0] converts from 2D array back to 1D array\n",
    "    print(\"Channel names: {channelNames}\".format(channelNames=chnames))\n",
    "    markers = o['marker']\n",
    "    ## The markers are all still individual arrays of size 1x1, so we convert them to an array with single values\n",
    "    markersArray = []\n",
    "    for marker in markers:\n",
    "        markersArray.append(marker[0])\n",
    "    markersArray = np.asarray(markersArray)\n",
    "    #For this dataset, the markers are 0, 1, or 2.\n",
    "    # 1 - Left Hand MI, 2 - Right Hand MI, 3 - Passive State, 0 - Rest (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def GetFrequentLengthSize(indeces, data):\n",
    "    lengthIndeces = [];\n",
    "    for index in indeces:\n",
    "        lengthIndeces.append(data[index].shape[1])\n",
    "    (values, counts) = np.unique(lengthIndeces, return_counts=True)\n",
    "    #print(np.argmax(counts))\n",
    "    ind = np.argmax(counts);\n",
    "    return np.asarray([values[ind], ind]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of index changes: 19110\n",
      "Number of arrays in data split: 19110\n",
      "Number of marker targets: 19110\n"
     ]
    }
   ],
   "source": [
    "## Find the starting indeces where the marker changes\n",
    "changeIdxs = np.where(np.transpose(markersArray)[:-1] != np.transpose(markersArray)[1:])[0]\n",
    "print(\"Number of index changes: {idxChanges}\".format(idxChanges=changeIdxs.shape[0]))\n",
    "## Split the data so that it has its matching marker\n",
    "dataSplit = np.array_split(data, changeIdxs[:-1], axis=1)\n",
    "splitCount = 0\n",
    "for splitData in dataSplit:\n",
    "    splitCount += 1\n",
    "print(\"Number of arrays in data split: {num}\".format(num=splitCount))\n",
    "## Retrieve the marker values for each of the change indeces (changeIdxs)\n",
    "markerTargets = markersArray[changeIdxs];\n",
    "print(\"Number of marker targets: {numTargets}\".format(numTargets=markerTargets.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5, 91, 92, 99], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(markerTargets)\n",
    "# The 5F data has the following marker targets:\n",
    "# 0 - ???\n",
    "# 1 - Thumb MI\n",
    "# 2 - Index finger MI\n",
    "# 3 - Middle finger MI\n",
    "# 4 - Ring finger MI\n",
    "# 5 - Pinkie finger MI\n",
    "# 91 - Inter-session rest break\n",
    "# 92 - Experiment end\n",
    "# 99 - Initial relaxation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Apply CSP, we first only get the indeces for MI tasks 1 and 2 (left and right hand, respectively.)\n",
    "thumbIdxs = np.where(markerTargets == 1)[0]\n",
    "indexIdxs = np.where(markerTargets == 2)[0]\n",
    "middleIdxs = np.where(markerTargets == 3)[0]\n",
    "ringIdxs = np.where(markerTargets == 4)[0]\n",
    "pinkieIdxs = np.where(markerTargets == 5)[0]\n",
    "maxsteps=220 #most actions have lengths < 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(thumbValues, thumbCounts) = np.unique(thumbIdxs, return_counts=True)\n",
    "ind = np.argmax(thumbCounts)\n",
    "print(thumbValues[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296 21\n",
      "1295 18\n",
      "1295 20\n",
      "1295 21\n",
      "1295 19\n"
     ]
    }
   ],
   "source": [
    "#Have to find the most common length of all of the experiments.\n",
    "(frequentThumbLength, frequentThumbCount) = GetFrequentLengthSize(thumbIdxs, dataSplit);\n",
    "(frequentIndexLength, frequentIndexCount) = GetFrequentLengthSize(indexIdxs, dataSplit);\n",
    "(frequentMiddleLength, frequentMiddleCount) = GetFrequentLengthSize(middleIdxs, dataSplit);\n",
    "(frequentRingLength, frequentRingCount) = GetFrequentLengthSize(ringIdxs, dataSplit);\n",
    "(frequentPinkieLength, frequentPinkieCount) = GetFrequentLengthSize(pinkieIdxs,dataSplit);\n",
    "print(frequentThumbLength, frequentThumbCount);\n",
    "print(frequentIndexLength, frequentIndexCount);\n",
    "print(frequentMiddleLength, frequentMiddleCount);\n",
    "print(frequentRingLength, frequentRingCount);\n",
    "print(frequentPinkieLength, frequentPinkieCount);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(lengthIndeces)\n",
    "print(np.argmax(counts))\n",
    "mostFreqCount = np.argmax(counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-7fd9c83f18d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get the indeces of all data points that have more than mostFreqCount number of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataSplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthumbIdxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "#Get the indeces of all data points that have more than mostFreqCount number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5490, 220, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftData = [];\n",
    "for leftIndex in LeftIdxs[0]:\n",
    "    #print(leftIndex)\n",
    "    #print(\"Dimensions of index: {ind}\".format(ind=dataSplit[leftIndex].shape))\n",
    "    if(dataSplit[leftIndex].shape[1] > maxsteps):\n",
    "        continue\n",
    "    else:\n",
    "        diff=maxsteps-dataSplit[leftIndex].shape[1]\n",
    "        temp=np.pad(dataSplit[leftIndex], ((0,0),(0,diff)), mode='constant')\n",
    "        leftData.append(np.transpose(temp))\n",
    "leftData = np.asarray(leftData)\n",
    "leftData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5605, 220, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightData = [];\n",
    "for rightIndex in RightIdxs[0]:\n",
    "    #print(leftIndex)\n",
    "    #print(\"Dimensions of index: {ind}\".format(ind=dataSplit[leftIndex].shape))\n",
    "    if(dataSplit[rightIndex].shape[1] > maxsteps):\n",
    "        continue\n",
    "    else:\n",
    "        diff=maxsteps-dataSplit[rightIndex].shape[1]\n",
    "        temp=np.pad(dataSplit[rightIndex], ((0,0),(0,diff)), mode='constant')\n",
    "        rightData.append(np.transpose(temp))\n",
    "rightData = np.asarray(rightData)\n",
    "rightData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only keep the top 5490 samples, so that left and right data are equal\n",
    "rightDataSub = rightData[1:5491]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrData Shape: (10980, 220, 21)\tmarkerTargets Shape: (10980, 2)\n"
     ]
    }
   ],
   "source": [
    "#Construct the target array and merge the data\n",
    "leftTargets = np.tile(np.array([1,0]),(5490,1))\n",
    "rightTargets = np.tile(np.array([0,1]), (5490,1))\n",
    "markerTargets = np.vstack((leftTargets, rightTargets))\n",
    "lrData = np.vstack((leftData, rightDataSub))\n",
    "\n",
    "#Sanity Check\n",
    "print(\"lrData Shape: {arg1}\\tmarkerTargets Shape: {arg2}\".format(arg1=lrData.shape, arg2=markerTargets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct LSTM using Tensorflow + Keras\n",
    "# Import Libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle the data\n",
    "lrData, markerTargets = shuffle(lrData, markerTargets, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7686, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split into train and test sets\n",
    "lrDataTrain, lrDataTest, markerTargetsTrain, markerTargetsTest = train_test_split(lrData, markerTargets, test_size=0.3, random_state=1)\n",
    "markerTargetsTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape the data for time-series processing\n",
    "## Syntax np.reshape((numExamples, numTimeSteps, numInputs/numFeatures))\n",
    "lrDataTrainRe = lrDataTrain.reshape((lrDataTrain.shape[0], lrDataTrain.shape[1], lrDataTrain.shape[2]))\n",
    "lrDataTestRe = lrDataTest.reshape((lrDataTest.shape[0], lrDataTest.shape[1], lrDataTest.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "## Construct the model\n",
    "LSTM_EEG = Sequential()\n",
    "LSTM_EEG.add(LSTM((100),batch_input_shape=(None,lrDataTrainRe.shape[1], lrDataTrainRe.shape[2]), return_sequences=True))\n",
    "LSTM_EEG.add(LSTM((50), return_sequences=False))\n",
    "LSTM_EEG.add(Dense((2),activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 220, 100)          48800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 79,102\n",
      "Trainable params: 79,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "LSTM_EEG.summary()\n",
    "sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "LSTM_EEG.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7686/7686 - 26s - loss: 0.6938 - acc: 0.5083\n",
      "Epoch 2/5\n",
      "7686/7686 - 26s - loss: 0.6930 - acc: 0.5184\n",
      "Epoch 3/5\n",
      "7686/7686 - 27s - loss: 0.6903 - acc: 0.5387\n",
      "Epoch 4/5\n",
      "7686/7686 - 27s - loss: 0.6880 - acc: 0.5405\n",
      "Epoch 5/5\n",
      "7686/7686 - 26s - loss: 0.6851 - acc: 0.5443\n"
     ]
    }
   ],
   "source": [
    "history = LSTM_EEG.fit(lrDataTrain, markerTargetsTrain, epochs=5,verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTest = LSTM_EEG.predict(lrDataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTest[predictionsTest>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTest[predictionsTest <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisonArrayTest = predictionsTest == markerTargetsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctCountTest = 0\n",
    "for boolValues in comparisonArrayTest:\n",
    "    if(boolValues[0] & boolValues[1]):\n",
    "        correctCountTest += 1\n",
    "falseCountTest = lrDataTest.shape[0] - correctCountTest\n",
    "\n",
    "predictionsTrain = LSTM_EEG.predict(lrDataTrain)\n",
    "predictionsTrain[predictionsTrain>0.5] = 1;\n",
    "predictionsTrain[predictionsTrain<=0.5] = 0;\n",
    "comparisonArrayTrain = predictionsTrain == markerTargetsTrain;\n",
    "\n",
    "correctCountTrain = 0\n",
    "for boolValues in comparisonArrayTrain:\n",
    "    if(boolValues[0] & boolValues[1]):\n",
    "        correctCountTrain += 1\n",
    "falseCountTrain = lrDataTrain.shape[0] - correctCountTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "#################################\n",
      "Training Performance:\n",
      "Correct MI Prediction: 4189\n",
      "Incorrect MI Prediction: 3497\n",
      "Percent Accuracy: 54.502%\n",
      "#################################\n",
      "#################################\n",
      "Testing Performance:\n",
      "Correct MI Prediction: 1674\n",
      "Incorrect MI Prediction: 1620\n",
      "Percent Accuracy: 50.820%\n",
      "#################################\n",
      "#################################\n"
     ]
    }
   ],
   "source": [
    "print(\"#################################\")\n",
    "print(\"#################################\")\n",
    "print(\"Training Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTrain, falseCountTrain, (correctCountTrain*100/lrDataTrain.shape[0])))\n",
    "print(\"#################################\")\n",
    "print(\"#################################\")\n",
    "print(\"Testing Performance:\\nCorrect MI Prediction: {}\\nIncorrect MI Prediction: {}\\nPercent Accuracy: {:.3f}%\".format(correctCountTest, falseCountTest, (correctCountTest*100/lrDataTest.shape[0])))\n",
    "print(\"#################################\")\n",
    "print(\"#################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
